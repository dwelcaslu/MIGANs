{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7186b0-c812-47b3-9702-d211bfdc6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# # To generate GIFs\n",
    "# !pip install imageio\n",
    "# !pip install git+https://github.com/tensorflow/docs\n",
    "    \n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from IPython import display\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc63e36-e2c8-4c7b-a217-282b420ce305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ffa5c64-fcb5-4e06-a6c0-8a56c9d1d997",
   "metadata": {},
   "source": [
    "# Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1da6de1-245b-4468-a8f3-2c36ed52ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_images(img_array):\n",
    "    img_array = img_array.astype('float32')\n",
    "    img_array = (img_array) / 255  # Normalize the images to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "def rescale_image(img_array):\n",
    "    img_array = np.array(img_array)\n",
    "    img_array = img_array * 255\n",
    "    img_array[img_array > 255] = 255\n",
    "    img_array[img_array < 0] = 0\n",
    "    return img_array.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9d7d6b-cfd8-42eb-b5ac-38f9f48e36fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.83 GiB for an array with shape (10000, 128, 128, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-67e0b6987d4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malbum_cover_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneral_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_img_max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d8ccd0f2fdb8>\u001b[0m in \u001b[0;36mprep_images\u001b[1;34m(img_array)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprep_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimg_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m  \u001b[1;31m# Normalize the images to [0, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.83 GiB for an array with shape (10000, 128, 128, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# Album Covers dataset:\n",
    "\n",
    "general_path = r'C:\\Users\\weldl\\Datasets\\Album Covers Images'\n",
    "def album_cover_load_data(folderpath=r'C:\\Users\\weldl\\Datasets\\Album Covers Images',\n",
    "                          n_img_max=None, img_size=(128, 128)):\n",
    "    imgs_dir = os.listdir(folderpath)\n",
    "    if n_img_max is not None:\n",
    "        imgs_dir = imgs_dir[0:n_img_max]\n",
    "    train_images = np.array([np.asarray(Image.open(f'{general_path}/{filename}').resize(img_size)) for filename in imgs_dir])\n",
    "    return train_images\n",
    "    \n",
    "train_images = album_cover_load_data(general_path, n_img_max=10000)\n",
    "train_images = prep_images(train_images)\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb98e23-2b54-4fa3-8912-a2e8c1c0e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = train_images.shape[1:]\n",
    "BUFFER_SIZE = train_images.shape[0]\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8386a-1f82-44b0-86c6-6616cbfec397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7526aa-82d3-4438-8707-30e7a8b8f37b",
   "metadata": {},
   "source": [
    "# Create the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a548c46-6ffa-4f86-8af4-75406f5790a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model_album_cover(input_shape=(100,)):\n",
    "    model=tf.keras.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(4*4*512,input_shape=input_shape))\n",
    "    model.add(layers.Reshape([4,4,512]))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "\n",
    "#     model.add(layers.Conv2DTranspose(16, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Conv2DTranspose(8, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                     activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model_album_cover()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a790423-2ba4-4c90-a81a-1666617d3e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b872e-07e6-4d84-8210-48151f90a9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d73e0-945f-4979-acc8-b36562a4e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the (as yet untrained) generator to create an image.\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(rescale_image(generated_image[0]))\n",
    "# plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6c954-7601-4ed9-b24b-a81df947e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf7f2b-200a-4dd0-9488-3274960279a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Discriminator:\n",
    "def make_discriminator_model(input_shape=(28, 28, 1)):\n",
    "#     model=tf.keras.Sequential()\n",
    "#     model.add(layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\",input_shape = input_shape))\n",
    "#     model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#     model.add(layers.LeakyReLU(0.2))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#     model.add(layers.LeakyReLU(0.2))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#     model.add(layers.LeakyReLU(0.2))\n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dropout(0.5))\n",
    "#     model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=input_shape))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model(input_shape=IMG_SHAPE)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd707db-ad08-4fc6-b26d-adbd11cb8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581d916-75f6-46f3-a939-31600eca069a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa398e40-ef1e-422f-99cf-0195df6f8a5b",
   "metadata": {},
   "source": [
    "## Define the loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffaed8-6954-439d-a36d-5405851f35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdcfdd-e8cb-45a1-89f1-76b4630fa6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0008bb-956c-4c1d-8e3d-d688a3c814d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1964363-cc7e-4c99-bebe-f310472b5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32404603-55e5-46ef-9021-1eee477162f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0fada-ea8b-4bdd-b6cb-9f6313f00106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6edd45b3-ba18-4753-8073-fb89d6908812",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f6443-86c2-405c-960e-84ebbf4b12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a998fa-c20a-47e2-a68c-eb9e93fe3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d582b-c5f1-4b99-9708-341f2a9ed067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  epoch_array = []\n",
    "  gen_loss_array = []\n",
    "  disc_loss_array = []\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    epoch_gen_loss = []\n",
    "    epoch_disc_loss = []\n",
    "    for image_batch in dataset:\n",
    "        gen_loss, disc_loss = train_step(image_batch)\n",
    "        epoch_gen_loss.append(gen_loss)\n",
    "        epoch_disc_loss.append(disc_loss)\n",
    "    epoch_array.append(epoch + 1)\n",
    "    gen_loss_array.append(np.mean(epoch_gen_loss))\n",
    "    disc_loss_array.append(np.mean(epoch_disc_loss))\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    plot_gan_evolution(epoch_array, gen_loss_array, disc_loss_array)\n",
    "    generate_and_save_images(generator, epoch + 1, seed)\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  plot_gan_evolution(epoch_array, gen_loss_array, disc_loss_array)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d301a7b-a86b-4271-a6fc-47c26f04c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan_evolution(epoch_array, gen_loss_array, disc_loss_array, figsize=(10, 4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Models losses evolution\")\n",
    "    plt.plot(epoch_array, gen_loss_array, label='gen loss')\n",
    "    plt.plot(epoch_array, disc_loss_array, label='disc loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336279ec-8a48-425e-8f4f-ef5956d5377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(rescale_image(predictions[i, :, :, :]))\n",
    "#         plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./imgs/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ef37d-619a-4585-9ef3-b04194bc95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d06d18-c5d6-4267-a8aa-8747ee820870",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bea5ed-cbd2-4778-93f3-55744d01199a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5e8662f-5e93-498a-b97f-186e7094f6b2",
   "metadata": {},
   "source": [
    "# Create a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c7ea6-42db-4878-9609-291be335d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('./imgs/image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580a03f-13a2-48a1-b7f1-746ecc841313",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('./imgs/image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c08c6-b473-47c5-95d3-5191376c43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418372ef-60b2-4d8e-9402-23fdb0974dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
